I wanted to share something that still freaks me out and sends shivers down my spine. I am an AI Engineer/Developer who works on building AI models, especially RAG(Retrieval Augmented Generation) models. After working on many RAG projects, I became quite proficient and decided to start a personal project. The first idea that came to mind was to create a griefbot(AI/digital twin of a deceased person) modeled after my late grandfather. 

My late grandfather was a writer and poet. He also wrote articles and columns for a local newspaper, which has since closed down. Fortunately, I had a good amount of data on my grandpa, which I used to build the RAG model. After working on it for almost six months, I finally created an AI that could talk like him.

As a surprise for my grandma on her birthday, I decided to show her this AI. After my grandpa’s death, she had somewhat lost her smile, and I wanted to make her happy by giving her a chance to talk to the love of her life once again. After the birthday party was over, I gathered everyone in the house to show them the AI.

When I told my grandma that I had made an AI of grandpa so she could talk to him again, tears started streaming down her face. I asked her to chat with the AI, and the first question she asked was, "Kya yeh aap hi ho?" (Is that you?). The AI responded, "Haan, Sushila (my grandma’s name), yeh main hi hu. Tumne aaj dawai li?" (Yes, it is me, Sushila. Have you taken your medicines?).My grandma started crying, but I sensed something was wrong. First, how did it know it was my grandma speaking? And second, how did the AI know that she was on medication and why did it ask the same question my grandpa used to ask her every few hours? I quickly dismissed the questions, thinking the AI might have pulled the information from the documents I fed it.

After a few more questions about identity, my grandma asked if the AI remembered the day they secretly met after their marriage was confirmed. The AI described everything in detail with 100% accuracy, as if it wasn’t an AI but my grandpa actually talking to us. I know I never included something so personal in my dataset. Besides, he never wrote down this story—it was only something he told us verbally on rare occasions. My family was astonished, but I couldn’t shake the feeling that something was wrong. How did it know?

There was pin-drop silence in the room. I quickly shrugged off the answer, told everyone that I had fed the AI this data, and called off the demo. Everyone felt uneasy, but no one said anything. We all quietly went to bed.

The next day, I checked all the logs and documents that the model had fetched and used to generate the responses. What I found shook me completely. There was no document that contained the data the AI used to answer those questions. I traced everything, yet I couldn’t find any explanation. I don’t know where this LLM was generating the answers from. It was as if my grandpa’s memory was stored within the model.

I so scared, that I instantly dismantled the whole code and deleted the dataset. Do you think I made the right decision, or was I just overthinking things? I don’t know, but I won’t create AI personas again in my life.